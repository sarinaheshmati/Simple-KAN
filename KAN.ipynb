{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca60gi7sS4Vy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ce7k6NtIc0O"
      },
      "outputs": [],
      "source": [
        "class PostActivationSet(nn.Module):\n",
        "    def __init__(self, input_size, grid_size, k, b, grid):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.k = k\n",
        "        self.b = b()\n",
        "        self.c = nn.Parameter(torch.normal(0, 0.1, size=(input_size, grid_size + k)))\n",
        "        self.w_b = nn.Parameter(torch.ones(input_size))\n",
        "        self.w_s = nn.Parameter(torch.rand(input_size))\n",
        "        self.grid = grid\n",
        "\n",
        "    def b_spline(self, t, i, p=0):\n",
        "        if p == 0:\n",
        "            return ((self.grid[i] <= t) & (t < self.grid[i + 1])).int()\n",
        "        return ((t - self.grid[i]) / (self.grid[i + p] - self.grid[i])) * self.b_spline(t, i, p - 1) + ((self.grid[i + p + 1] - t) / (self.grid[i + p + 1] - self.grid[i + 1])) * self.b_spline(t, i + 1, p - 1)\n",
        "\n",
        "    def b_splines(self, x):\n",
        "        bases = torch.tensor([self.b_spline(x[0].item(), i) for i in range(self.grid_size + self.k)])\n",
        "        for t in x[1:]:\n",
        "            bases = torch.cat((bases, torch.tensor([self.b_spline(t.item(), i) for i in range(self.grid_size + self.k)])))\n",
        "        return bases.view(x.shape[0], -1)\n",
        "\n",
        "    def spline(self, x):\n",
        "        return torch.sum(self.c * self.b_splines(x), dim=1).view(1, -1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sum(self.w_b * self.b(x) + self.w_s * self.spline(x)).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9oA6ARhwrGa"
      },
      "outputs": [],
      "source": [
        "class Layer(nn.Module):\n",
        "    def __init__(self, num_input, num_output, grid_size, k=3, b=nn.SiLU, grid_range=(-1, 1)):\n",
        "        super().__init__()\n",
        "        self.num_input = num_input\n",
        "        self.num_output = num_output\n",
        "        grid_interval = (grid_range[1] - grid_range[0]) / grid_size + grid_range[0]\n",
        "        grid = torch.arange(-k, grid_size + k + 1) * grid_interval + grid_range[0]\n",
        "        self.post_activation_sets = nn.ModuleList()\n",
        "        for i in range(num_output):\n",
        "            self.post_activation_sets.append(PostActivationSet(num_input, grid_size, k, b, grid))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.zeros(self.num_output)\n",
        "        for i in range(self.num_output):\n",
        "            out[i] += self.post_activation_sets[i](x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4qJRUZbwuAi"
      },
      "outputs": [],
      "source": [
        "class KAN(nn.Module):\n",
        "    def __init__(self, layer_sizes, grid_size, k=3, b=nn.SiLU, grid_range=(-1, 1)):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for num_input, num_output in zip(layer_sizes, layer_sizes[1:]):\n",
        "            self.layers.append(Layer(num_input, num_output, grid_size, k, b, grid_range))\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            unbinded_X = torch.unbind(X, dim=0)\n",
        "            X = torch.stack([layer(x) for i, x in enumerate(X)], dim=0)\n",
        "        return torch.sigmoid(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeWguDg3ZWBp",
        "outputId": "f4298158-acda-4905-fd85-af650a30a51a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-19 20:16:42--  https://archive.ics.uci.edu/static/public/151/connectionist+bench+sonar+mines+vs+rocks.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘connectionist+bench+sonar+mines+vs+rocks.zip.1’\n",
            "\n",
            "\r",
            "          connectio     [<=>                 ]       0  --.-KB/s               \r",
            "connectionist+bench     [ <=>                ]  63.88K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-06-19 20:16:42 (3.10 MB/s) - ‘connectionist+bench+sonar+mines+vs+rocks.zip.1’ saved [65413]\n",
            "\n",
            "Archive:  /content/connectionist+bench+sonar+mines+vs+rocks.zip\n",
            "replace sonar.all-data? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace sonar.mines? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace sonar.rocks? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace Index? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace sonar.names? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/151/connectionist+bench+sonar+mines+vs+rocks.zip\n",
        "!unzip /content/connectionist+bench+sonar+mines+vs+rocks.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "964tzzYRZHlz",
        "outputId": "6fde87db-d795-4780-e6f7-e4be16b41afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.3500e-02, 4.5000e-03, 5.1000e-03, 2.8900e-02, 5.6100e-02, 9.2900e-02,\n",
            "         1.0310e-01, 8.8300e-02, 1.5960e-01, 1.9080e-01, 1.5760e-01, 1.1120e-01,\n",
            "         1.1970e-01, 1.1740e-01, 1.4150e-01, 2.2150e-01, 2.6580e-01, 2.7130e-01,\n",
            "         3.8620e-01, 5.7170e-01, 6.7970e-01, 8.7470e-01, 1.0000e+00, 8.9480e-01,\n",
            "         8.4200e-01, 9.1740e-01, 9.3070e-01, 9.0500e-01, 8.2280e-01, 6.9860e-01,\n",
            "         5.8310e-01, 4.9240e-01, 4.5630e-01, 5.1590e-01, 5.6700e-01, 5.2840e-01,\n",
            "         5.1440e-01, 3.7420e-01, 2.2820e-01, 1.1930e-01, 1.0880e-01, 4.3100e-02,\n",
            "         1.0700e-01, 5.8300e-02, 4.6000e-03, 4.7300e-02, 4.0800e-02, 2.9000e-02,\n",
            "         1.9200e-02, 9.4000e-03, 2.5000e-03, 3.7000e-03, 8.4000e-03, 1.0200e-02,\n",
            "         9.6000e-03, 2.4000e-03, 3.7000e-03, 2.8000e-03, 3.0000e-03, 3.0000e-03],\n",
            "        [2.6500e-02, 4.4000e-02, 1.3700e-02, 8.4000e-03, 3.0500e-02, 4.3800e-02,\n",
            "         3.4100e-02, 7.8000e-02, 8.4400e-02, 7.7900e-02, 3.2700e-02, 2.0600e-01,\n",
            "         1.9080e-01, 1.0650e-01, 1.4570e-01, 2.2320e-01, 2.0700e-01, 1.1050e-01,\n",
            "         1.0780e-01, 1.1650e-01, 2.2240e-01, 6.8900e-02, 2.0600e-01, 2.3840e-01,\n",
            "         9.0400e-02, 2.2780e-01, 5.8720e-01, 8.4570e-01, 8.4670e-01, 7.6790e-01,\n",
            "         8.0550e-01, 6.2600e-01, 6.5450e-01, 8.7470e-01, 9.8850e-01, 9.3480e-01,\n",
            "         6.9600e-01, 5.7330e-01, 5.8720e-01, 6.6630e-01, 5.6510e-01, 5.2470e-01,\n",
            "         3.6840e-01, 1.9970e-01, 1.5120e-01, 5.0800e-02, 9.3100e-02, 9.8200e-02,\n",
            "         5.2400e-02, 1.8800e-02, 1.0000e-02, 3.8000e-03, 1.8700e-02, 1.5600e-02,\n",
            "         6.8000e-03, 9.7000e-03, 7.3000e-03, 8.1000e-03, 8.6000e-03, 9.5000e-03],\n",
            "        [2.6200e-02, 5.8200e-02, 1.0990e-01, 1.0830e-01, 9.7400e-02, 2.2800e-01,\n",
            "         2.4310e-01, 3.7710e-01, 5.5980e-01, 6.1940e-01, 6.3330e-01, 7.0600e-01,\n",
            "         5.5440e-01, 5.3200e-01, 6.4790e-01, 6.9310e-01, 6.7590e-01, 7.5510e-01,\n",
            "         8.9290e-01, 8.6190e-01, 7.9740e-01, 6.7370e-01, 4.2930e-01, 3.6480e-01,\n",
            "         5.3310e-01, 2.4130e-01, 5.0700e-01, 8.5330e-01, 6.0360e-01, 8.5140e-01,\n",
            "         8.5120e-01, 5.0450e-01, 1.8620e-01, 2.7090e-01, 4.2320e-01, 3.0430e-01,\n",
            "         6.1160e-01, 6.7560e-01, 5.3750e-01, 4.7190e-01, 4.6470e-01, 2.5870e-01,\n",
            "         2.1290e-01, 2.2220e-01, 2.1110e-01, 1.7600e-02, 1.3480e-01, 7.4400e-02,\n",
            "         1.3000e-02, 1.0600e-02, 3.3000e-03, 2.3200e-02, 1.6600e-02, 9.5000e-03,\n",
            "         1.8000e-02, 2.4400e-02, 3.1600e-02, 1.6400e-02, 9.5000e-03, 7.8000e-03],\n",
            "        [1.8900e-02, 3.0800e-02, 1.9700e-02, 6.2200e-02, 8.0000e-03, 7.8900e-02,\n",
            "         1.4400e-01, 1.4510e-01, 1.7890e-01, 2.5220e-01, 2.6070e-01, 3.7100e-01,\n",
            "         3.9060e-01, 2.6720e-01, 2.7160e-01, 4.1830e-01, 6.9880e-01, 5.7330e-01,\n",
            "         2.2260e-01, 2.6310e-01, 7.4730e-01, 7.2630e-01, 3.3930e-01, 2.8240e-01,\n",
            "         6.0530e-01, 5.8970e-01, 4.9670e-01, 8.6160e-01, 8.3390e-01, 4.0840e-01,\n",
            "         2.2680e-01, 1.7450e-01, 5.0700e-02, 1.5880e-01, 3.0400e-01, 1.3690e-01,\n",
            "         1.6050e-01, 2.0610e-01, 7.3400e-02, 2.0200e-02, 1.6380e-01, 1.5830e-01,\n",
            "         1.8300e-01, 1.8860e-01, 1.0080e-01, 6.6300e-02, 1.8300e-02, 4.0400e-02,\n",
            "         1.0800e-02, 1.4300e-02, 9.1000e-03, 3.8000e-03, 9.6000e-03, 1.4200e-02,\n",
            "         1.9000e-02, 1.4000e-02, 9.9000e-03, 9.2000e-03, 5.2000e-03, 7.5000e-03],\n",
            "        [2.6400e-02, 7.1000e-03, 3.4200e-02, 7.9300e-02, 1.0430e-01, 7.8300e-02,\n",
            "         1.4170e-01, 1.1760e-01, 4.5300e-02, 9.4500e-02, 1.1320e-01, 8.4000e-02,\n",
            "         7.1700e-02, 1.9680e-01, 2.6330e-01, 4.1910e-01, 5.0500e-01, 6.7110e-01,\n",
            "         7.9220e-01, 8.3810e-01, 8.7590e-01, 9.4220e-01, 1.0000e+00, 9.9310e-01,\n",
            "         9.5750e-01, 8.6470e-01, 7.2150e-01, 5.8010e-01, 4.9640e-01, 4.8860e-01,\n",
            "         4.0790e-01, 2.4430e-01, 1.7680e-01, 2.4720e-01, 3.5180e-01, 3.7620e-01,\n",
            "         2.9090e-01, 2.3110e-01, 3.1680e-01, 3.5540e-01, 3.7410e-01, 4.4430e-01,\n",
            "         3.2610e-01, 1.9630e-01, 8.6400e-02, 1.6880e-01, 1.9910e-01, 1.2170e-01,\n",
            "         6.2800e-02, 3.2300e-02, 2.5300e-02, 2.1400e-02, 2.6200e-02, 1.7700e-02,\n",
            "         3.7000e-03, 6.8000e-03, 1.2100e-02, 7.7000e-03, 7.8000e-03, 6.6000e-03],\n",
            "        [1.1600e-02, 7.4400e-02, 3.6700e-02, 2.2500e-02, 7.6000e-03, 5.4500e-02,\n",
            "         1.1100e-01, 1.0690e-01, 1.7080e-01, 2.2710e-01, 3.1710e-01, 2.8820e-01,\n",
            "         2.6570e-01, 2.3070e-01, 1.8890e-01, 1.7910e-01, 2.2980e-01, 3.7150e-01,\n",
            "         6.2230e-01, 7.2600e-01, 7.9340e-01, 8.0450e-01, 8.0670e-01, 9.1730e-01,\n",
            "         9.3270e-01, 9.5620e-01, 1.0000e+00, 9.8180e-01, 8.6840e-01, 6.3810e-01,\n",
            "         3.9970e-01, 3.2420e-01, 2.8350e-01, 2.4130e-01, 2.3210e-01, 1.2600e-01,\n",
            "         6.9300e-02, 7.0100e-02, 1.4390e-01, 1.4750e-01, 4.3800e-02, 4.6900e-02,\n",
            "         1.4760e-01, 1.7420e-01, 1.5550e-01, 1.6510e-01, 1.1810e-01, 7.2000e-02,\n",
            "         3.2100e-02, 5.6000e-03, 2.0200e-02, 1.4100e-02, 1.0300e-02, 1.0000e-02,\n",
            "         3.4000e-03, 2.6000e-03, 3.7000e-03, 4.4000e-03, 5.7000e-03, 3.5000e-03],\n",
            "        [2.2800e-02, 1.0600e-02, 1.3000e-02, 8.4200e-02, 1.1170e-01, 1.5060e-01,\n",
            "         1.7760e-01, 9.9700e-02, 1.4280e-01, 2.2270e-01, 2.6210e-01, 3.1090e-01,\n",
            "         2.8590e-01, 3.3160e-01, 3.7550e-01, 4.4990e-01, 4.7650e-01, 6.2540e-01,\n",
            "         7.3040e-01, 8.7020e-01, 9.3490e-01, 9.6140e-01, 9.1260e-01, 9.4430e-01,\n",
            "         1.0000e+00, 9.4550e-01, 8.8150e-01, 7.5200e-01, 7.0680e-01, 5.9860e-01,\n",
            "         3.8570e-01, 2.5100e-01, 2.1620e-01, 9.6800e-02, 1.3230e-01, 1.3440e-01,\n",
            "         2.2500e-01, 3.2440e-01, 3.9390e-01, 3.8060e-01, 3.2580e-01, 3.6540e-01,\n",
            "         2.9830e-01, 1.7790e-01, 1.5350e-01, 1.1990e-01, 9.5900e-02, 7.6500e-02,\n",
            "         6.4900e-02, 3.1300e-02, 1.8500e-02, 9.8000e-03, 1.7800e-02, 7.7000e-03,\n",
            "         7.4000e-03, 9.5000e-03, 5.5000e-03, 4.5000e-03, 6.3000e-03, 3.9000e-03],\n",
            "        [3.0700e-02, 5.2300e-02, 6.5300e-02, 5.2100e-02, 6.1100e-02, 5.7700e-02,\n",
            "         6.6500e-02, 6.6400e-02, 1.4600e-01, 2.7920e-01, 3.8770e-01, 4.9920e-01,\n",
            "         4.9810e-01, 4.9720e-01, 5.6070e-01, 7.3390e-01, 8.2300e-01, 9.1730e-01,\n",
            "         9.9750e-01, 9.9110e-01, 8.2400e-01, 6.4980e-01, 5.9800e-01, 4.8620e-01,\n",
            "         3.1500e-01, 1.5430e-01, 9.8900e-02, 2.8400e-02, 1.0080e-01, 2.6360e-01,\n",
            "         2.6940e-01, 2.9300e-01, 2.9250e-01, 3.9980e-01, 3.6600e-01, 3.1720e-01,\n",
            "         4.6090e-01, 4.3740e-01, 1.8200e-01, 3.3760e-01, 6.2020e-01, 4.4480e-01,\n",
            "         1.8630e-01, 1.4200e-01, 5.8900e-02, 5.7600e-02, 6.7200e-02, 2.6900e-02,\n",
            "         2.4500e-02, 1.9000e-02, 6.3000e-03, 3.2100e-02, 1.8900e-02, 1.3700e-02,\n",
            "         2.7700e-02, 1.5200e-02, 5.2000e-03, 1.2100e-02, 1.2400e-02, 5.5000e-03],\n",
            "        [2.9900e-02, 6.8800e-02, 9.9200e-02, 1.0210e-01, 8.0000e-02, 6.2900e-02,\n",
            "         1.3000e-02, 8.1300e-02, 1.7610e-01, 9.9800e-02, 5.2300e-02, 9.0400e-02,\n",
            "         2.6550e-01, 3.0990e-01, 3.5200e-01, 3.8920e-01, 3.9620e-01, 2.4490e-01,\n",
            "         2.3550e-01, 3.0450e-01, 3.1120e-01, 4.6980e-01, 5.5340e-01, 4.5320e-01,\n",
            "         4.4640e-01, 4.6700e-01, 4.6210e-01, 6.9880e-01, 7.6260e-01, 7.0250e-01,\n",
            "         7.3820e-01, 7.4460e-01, 7.9270e-01, 5.2270e-01, 3.9670e-01, 3.0420e-01,\n",
            "         1.3090e-01, 2.4080e-01, 1.7800e-01, 1.5980e-01, 5.6570e-01, 6.4430e-01,\n",
            "         4.2410e-01, 4.5670e-01, 5.7600e-01, 5.2930e-01, 3.2870e-01, 1.2830e-01,\n",
            "         6.9800e-02, 3.3400e-02, 3.4200e-02, 4.5900e-02, 2.7700e-02, 1.7200e-02,\n",
            "         8.7000e-03, 4.6000e-03, 2.0300e-02, 1.3000e-02, 1.1500e-02, 1.5000e-03],\n",
            "        [2.4000e-02, 2.1800e-02, 3.2400e-02, 5.6900e-02, 3.3000e-02, 5.1300e-02,\n",
            "         8.9700e-02, 7.1300e-02, 5.6900e-02, 3.8900e-02, 1.9340e-01, 2.4340e-01,\n",
            "         2.9060e-01, 2.6060e-01, 3.8110e-01, 4.9970e-01, 3.0150e-01, 3.6550e-01,\n",
            "         6.7910e-01, 7.3070e-01, 5.0530e-01, 4.4410e-01, 6.9870e-01, 8.1330e-01,\n",
            "         7.7810e-01, 8.9430e-01, 8.9290e-01, 8.9130e-01, 8.6100e-01, 8.0630e-01,\n",
            "         5.5400e-01, 2.4460e-01, 3.4590e-01, 1.6150e-01, 2.4670e-01, 5.5640e-01,\n",
            "         4.6810e-01, 9.7900e-02, 1.5820e-01, 7.5100e-02, 3.3210e-01, 3.7450e-01,\n",
            "         2.6660e-01, 1.0780e-01, 1.4180e-01, 1.6870e-01, 7.3800e-02, 6.3400e-02,\n",
            "         1.4400e-02, 2.2600e-02, 6.1000e-03, 1.6200e-02, 1.4600e-02, 9.3000e-03,\n",
            "         1.1200e-02, 9.4000e-03, 5.4000e-03, 1.9000e-03, 6.6000e-03, 2.3000e-03],\n",
            "        [2.1600e-02, 2.1500e-02, 2.7300e-02, 1.3900e-02, 3.5700e-02, 7.8500e-02,\n",
            "         9.0600e-02, 9.0800e-02, 1.1510e-01, 9.7300e-02, 1.2030e-01, 1.1020e-01,\n",
            "         1.1920e-01, 1.7620e-01, 2.3900e-01, 2.1380e-01, 1.9290e-01, 1.7650e-01,\n",
            "         7.4600e-02, 1.2650e-01, 2.0050e-01, 1.5710e-01, 2.6050e-01, 5.3860e-01,\n",
            "         8.4400e-01, 1.0000e+00, 8.6840e-01, 6.7420e-01, 5.5370e-01, 4.6380e-01,\n",
            "         3.6090e-01, 2.0550e-01, 1.6200e-01, 2.0920e-01, 3.1000e-01, 2.3440e-01,\n",
            "         1.0580e-01, 3.8300e-02, 5.2800e-02, 1.2910e-01, 2.2410e-01, 1.9150e-01,\n",
            "         1.5870e-01, 9.4200e-02, 8.4000e-02, 6.7000e-02, 3.4200e-02, 4.6900e-02,\n",
            "         3.5700e-02, 1.3600e-02, 8.2000e-03, 1.4000e-02, 4.4000e-03, 5.2000e-03,\n",
            "         7.3000e-03, 2.1000e-03, 4.7000e-03, 2.4000e-03, 9.0000e-04, 1.7000e-03],\n",
            "        [1.0900e-02, 9.3000e-03, 1.2100e-02, 3.7800e-02, 6.7900e-02, 8.6300e-02,\n",
            "         1.0040e-01, 6.6400e-02, 9.4100e-02, 1.0360e-01, 9.7200e-02, 5.0100e-02,\n",
            "         1.5460e-01, 3.4040e-01, 4.8040e-01, 6.5700e-01, 7.7380e-01, 7.8270e-01,\n",
            "         8.1520e-01, 8.1290e-01, 8.2970e-01, 8.5350e-01, 8.8700e-01, 8.8940e-01,\n",
            "         8.9800e-01, 9.6670e-01, 1.0000e+00, 9.1340e-01, 6.7620e-01, 4.6590e-01,\n",
            "         2.8950e-01, 2.9590e-01, 1.7460e-01, 2.1120e-01, 2.5690e-01, 2.2760e-01,\n",
            "         2.1490e-01, 1.6010e-01, 3.7100e-02, 1.1700e-02, 4.8800e-02, 2.8800e-02,\n",
            "         5.9700e-02, 4.3100e-02, 3.6900e-02, 2.5000e-03, 3.2700e-02, 2.5700e-02,\n",
            "         1.8200e-02, 1.0800e-02, 1.2400e-02, 7.7000e-03, 2.3000e-03, 1.1700e-02,\n",
            "         5.3000e-03, 7.7000e-03, 7.6000e-03, 5.6000e-03, 5.5000e-03, 3.9000e-03],\n",
            "        [3.3100e-02, 4.2300e-02, 4.7400e-02, 8.1800e-02, 8.3500e-02, 7.5600e-02,\n",
            "         3.7400e-02, 9.6100e-02, 5.4800e-02, 1.9300e-02, 8.9700e-02, 1.7340e-01,\n",
            "         1.9360e-01, 2.8030e-01, 3.3130e-01, 5.0200e-01, 6.3600e-01, 7.0960e-01,\n",
            "         8.3330e-01, 8.7300e-01, 8.0730e-01, 7.5070e-01, 7.5260e-01, 7.2980e-01,\n",
            "         6.1770e-01, 4.9460e-01, 4.5310e-01, 4.0990e-01, 4.5400e-01, 4.1240e-01,\n",
            "         3.1390e-01, 3.1940e-01, 3.6920e-01, 3.7760e-01, 4.4690e-01, 4.7770e-01,\n",
            "         4.7160e-01, 4.6640e-01, 3.8930e-01, 4.2550e-01, 4.0640e-01, 3.7120e-01,\n",
            "         3.8630e-01, 2.8020e-01, 1.2830e-01, 1.1170e-01, 1.3030e-01, 7.8700e-02,\n",
            "         4.3600e-02, 2.2400e-02, 1.3300e-02, 7.8000e-03, 1.7400e-02, 1.7600e-02,\n",
            "         3.8000e-03, 1.2900e-02, 6.6000e-03, 4.4000e-03, 1.3400e-02, 9.2000e-03],\n",
            "        [2.0100e-02, 1.7800e-02, 2.7400e-02, 2.3200e-02, 7.2400e-02, 8.3300e-02,\n",
            "         1.2320e-01, 1.2980e-01, 2.0850e-01, 2.7200e-01, 2.1880e-01, 3.0370e-01,\n",
            "         2.9590e-01, 2.0590e-01, 9.0600e-02, 1.6100e-01, 1.8000e-01, 2.1800e-01,\n",
            "         2.0260e-01, 1.5060e-01, 5.2100e-02, 2.1430e-01, 4.3330e-01, 5.9430e-01,\n",
            "         6.9260e-01, 7.5760e-01, 8.7870e-01, 9.0600e-01, 8.5280e-01, 9.0870e-01,\n",
            "         9.6570e-01, 9.3060e-01, 7.7740e-01, 6.6430e-01, 6.6040e-01, 6.8840e-01,\n",
            "         6.9380e-01, 5.9320e-01, 5.7740e-01, 6.2230e-01, 5.8410e-01, 4.5270e-01,\n",
            "         4.9110e-01, 5.7620e-01, 5.0130e-01, 4.0420e-01, 3.1230e-01, 2.2320e-01,\n",
            "         1.0850e-01, 4.1400e-02, 2.5300e-02, 1.3100e-02, 4.9000e-03, 1.0400e-02,\n",
            "         1.0200e-02, 9.2000e-03, 8.3000e-03, 2.0000e-03, 4.8000e-03, 3.6000e-03],\n",
            "        [3.6800e-02, 2.7900e-02, 1.0300e-02, 5.6600e-02, 7.5900e-02, 6.7900e-02,\n",
            "         9.7000e-02, 1.4730e-01, 2.1640e-01, 2.5440e-01, 2.9360e-01, 2.9350e-01,\n",
            "         2.6570e-01, 3.1870e-01, 2.7940e-01, 2.5340e-01, 1.9800e-01, 1.9290e-01,\n",
            "         2.8260e-01, 3.2450e-01, 3.5040e-01, 3.3240e-01, 4.2170e-01, 4.7740e-01,\n",
            "         4.8080e-01, 6.3250e-01, 8.3340e-01, 9.4580e-01, 1.0000e+00, 8.4250e-01,\n",
            "         5.5240e-01, 4.7950e-01, 5.2000e-01, 3.9680e-01, 1.9400e-01, 1.5190e-01,\n",
            "         2.0100e-01, 1.7360e-01, 1.0290e-01, 2.2440e-01, 3.7170e-01, 4.4490e-01,\n",
            "         3.9390e-01, 2.0300e-01, 2.0100e-01, 2.1870e-01, 1.8400e-01, 1.4770e-01,\n",
            "         9.7100e-02, 2.2400e-02, 1.5100e-02, 1.0500e-02, 2.4000e-03, 1.8000e-03,\n",
            "         5.7000e-03, 9.2000e-03, 9.0000e-04, 8.6000e-03, 1.1000e-02, 5.2000e-03],\n",
            "        [1.0000e-02, 1.7100e-02, 6.2300e-02, 2.0500e-02, 2.0500e-02, 3.6800e-02,\n",
            "         1.0980e-01, 1.2760e-01, 5.9800e-02, 1.2640e-01, 8.8100e-02, 1.9920e-01,\n",
            "         1.8400e-02, 2.2610e-01, 1.7290e-01, 2.1310e-01, 6.9300e-02, 2.2810e-01,\n",
            "         4.0600e-01, 3.9730e-01, 2.7410e-01, 3.6900e-01, 5.5560e-01, 4.8460e-01,\n",
            "         3.1400e-01, 5.3340e-01, 5.2560e-01, 2.5200e-01, 2.0900e-01, 3.5590e-01,\n",
            "         6.2600e-01, 7.3400e-01, 6.1200e-01, 3.4970e-01, 3.9530e-01, 3.0120e-01,\n",
            "         5.4080e-01, 8.8140e-01, 9.8570e-01, 9.1670e-01, 6.1210e-01, 5.0060e-01,\n",
            "         3.2100e-01, 3.2020e-01, 4.2950e-01, 3.6540e-01, 2.6550e-01, 1.5760e-01,\n",
            "         6.8100e-02, 2.9400e-02, 2.4100e-02, 1.2100e-02, 3.6000e-03, 1.5000e-02,\n",
            "         8.5000e-03, 7.3000e-03, 5.0000e-03, 4.4000e-03, 4.0000e-03, 1.1700e-02]]) tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/sonar.all-data\", header=None)\n",
        "X = data.iloc[:, 0:60].values\n",
        "y = data.iloc[:, 60].values\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y = encoder.transform(y)\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "loader = DataLoader(list(zip(X,y)), shuffle=True, batch_size=16)\n",
        "for X_batch, y_batch in loader:\n",
        "    print(X_batch, y_batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1dkRyujah9W",
        "outputId": "cde9d652-9d36-4063-85ce-f8e50a999a50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KAN(\n",
              "  (layers): ModuleList(\n",
              "    (0): Layer(\n",
              "      (post_activation_sets): ModuleList(\n",
              "        (0-59): 60 x PostActivationSet(\n",
              "          (b): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): Layer(\n",
              "      (post_activation_sets): ModuleList(\n",
              "        (0-29): 30 x PostActivationSet(\n",
              "          (b): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Layer(\n",
              "      (post_activation_sets): ModuleList(\n",
              "        (0): PostActivationSet(\n",
              "          (b): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = KAN([60, 60, 30, 1], 5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9ljufPrZ0cD",
        "outputId": "84305e18-2a02-4a26-a260-f6ffd405ce9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 49.21%\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
        "\n",
        "loader = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=16)\n",
        "\n",
        "n_epochs = 10\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in loader:\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.requires_grad = True\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNLc9WhNVvAg"
      },
      "outputs": [],
      "source": [
        "class PostActivationSet(nn.Module):\n",
        "    def __init__(self, input_size, grid_size, k, b, grid_range):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.grid_size = grid_size\n",
        "        self.k = k\n",
        "        self.b = b()\n",
        "        self.c = nn.Parameter(torch.normal(0, 0.1, size=(input_size, grid_size + k)))\n",
        "        self.w_b = nn.Parameter(torch.ones(input_size))\n",
        "        self.w_s = nn.Parameter(torch.rand(input_size))\n",
        "        self.grid_range = grid_range\n",
        "        self.create_grid()\n",
        "\n",
        "    def create_grid(self):\n",
        "        grid_interval = (self.grid_range[1] - self.grid_range[0]) / self.grid_size + self.grid_range[0]\n",
        "        self.grid = (torch.arange(-self.k, self.grid_size + self.k + 1) * grid_interval + self.grid_range[0]).expand(self.input_size, -1).contiguous()\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the B-spline bases for the given input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: B-spline bases tensor of shape (batch_size, input_size, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.input_size\n",
        "\n",
        "        grid: torch.Tensor = (\n",
        "            self.grid\n",
        "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.k + 1):\n",
        "            bases = (\n",
        "                (x - grid[:, : -(k + 1)])\n",
        "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
        "                * bases[:, :, :-1]\n",
        "            ) + (\n",
        "                (grid[:, k + 1 :] - x)\n",
        "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
        "                * bases[:, :, 1:]\n",
        "            )\n",
        "\n",
        "        assert bases.size() == (\n",
        "            x.size(0),\n",
        "            self.input_size,\n",
        "            self.grid_size + self.k,\n",
        "        )\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def spline(self, X):\n",
        "        c = self.c.reshape(1, self.input_size, self.grid_size + self.k).expand(X.shape[0], self.input_size, -1)\n",
        "        return torch.sum(c * self.b_splines(X), dim=2).reshape(X.shape[0], -1)\n",
        "\n",
        "    def forward(self, X, grid_extension):\n",
        "        if grid_extension:\n",
        "            previous_spline = self.spline(X)\n",
        "            self.grid_size *= 2\n",
        "            self.create_grid()\n",
        "            b_splines_ = self.b_splines(X)\n",
        "            c = (torch.linalg.lstsq(b_splines_[:, 0, :], previous_spline[:, 0])).solution.reshape(1, -1)\n",
        "            for i in range(1, self.input_size):\n",
        "                c = torch.cat((c, (torch.linalg.lstsq(b_splines_[:, 0, :], previous_spline[:, 0])).solution.reshape(1, -1)))\n",
        "            self.c = nn.Parameter(c)\n",
        "        return torch.sum(self.w_b * self.b(X) + self.w_s * self.spline(X)).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNwVQSNu3-zB"
      },
      "outputs": [],
      "source": [
        "class Layer(nn.Module):\n",
        "    def __init__(self, num_input, num_output, grid_size, k=3, b=nn.SiLU, grid_range=(-1, 1)):\n",
        "        super().__init__()\n",
        "        self.num_input = num_input\n",
        "        self.num_output = num_output\n",
        "        grid_interval = (grid_range[1] - grid_range[0]) / grid_size + grid_range[0]\n",
        "        grid = torch.arange(-k, grid_size + k + 1) * grid_interval + grid_range[0]\n",
        "        self.post_activation_sets = nn.ModuleList()\n",
        "        for i in range(num_output):\n",
        "            self.post_activation_sets.append(PostActivationSet(num_input, grid_size, k, b, grid))\n",
        "\n",
        "    def forward(self, X, grid_extension):\n",
        "        out = torch.zeros(X.shape[0], self.num_output)\n",
        "        for i in range(self.num_output):\n",
        "            out[:, i] += self.post_activation_sets[i](X, grid_extension)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss7bhbpPie9v"
      },
      "outputs": [],
      "source": [
        "class KAN(nn.Module):\n",
        "    def __init__(self, layer_sizes, grid_size, k=3, b=nn.SiLU, grid_range=(-1, 1)):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for num_input, num_output in zip(layer_sizes, layer_sizes[1:]):\n",
        "            self.layers.append(Layer(num_input, num_output, grid_size, k, b, grid_range))\n",
        "\n",
        "    def forward(self, X, grid_extension=False):\n",
        "        for layer in self.layers:\n",
        "            X = layer(X, grid_extension)\n",
        "        return torch.sigmoid(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEF-5MCa2Ftc",
        "outputId": "42998cee-8222-4a3f-8b11-8c27b88edd07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KAN(\n",
              "  (layers): ModuleList(\n",
              "    (0): Layer(\n",
              "      (post_activation_sets): ModuleList(\n",
              "        (0-59): 60 x PostActivationSet(\n",
              "          (b): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): Layer(\n",
              "      (post_activation_sets): ModuleList(\n",
              "        (0-29): 30 x PostActivationSet(\n",
              "          (b): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Layer(\n",
              "      (post_activation_sets): ModuleList(\n",
              "        (0): PostActivationSet(\n",
              "          (b): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = KAN([60, 60, 30, 1], 5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C26tvdy2GeP"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
        "\n",
        "loader = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=16)\n",
        "\n",
        "n_epochs = 10\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in loader:\n",
        "        if epoch % 4 == 0:\n",
        "            y_pred = model(X_batch, True)\n",
        "        else:\n",
        "            y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.requires_grad = True\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}